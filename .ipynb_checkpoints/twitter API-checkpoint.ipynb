{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminary notebook to learn about API and Python integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/natalie/anaconda3/envs/insight/lib/python3.6/site-packages\")\n",
    "from twython import Twython\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save twitter specifics to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {}  \n",
    "credentials['CONSUMER_KEY'] = '4gd8qFAcXZVvOtipmFwuGVV5v'  \n",
    "credentials['CONSUMER_SECRET'] = 'R3gupzeBTifTXJUvpMDGTs61nfwpEd4EvlTB68b5lHZA6VYGML' \n",
    "credentials['ACCESS_TOKEN'] = '2882971510-d3OV32HpxWNVnC8XI87qS4uiaUISIolYOToui6N' \n",
    "credentials['ACCESS_SECRET'] = 'L12PhmS9uTl3sTw7L1G98PJmZOD4birfvc6EgPC3Z0uoE'\n",
    "\n",
    "# Save the credentials object to file\n",
    "with open(\"/home/natalie/Documents/Python_Training/twitterAPI/twitter_credentials.json\", \"w\") as file:  \n",
    "    json.dump(credentials, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize twitter\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:  \n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search tweitter for the tweets occuring in the previous 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
    "\n",
    "# Create our query to find the most popular tweets containing the words 'learn python'\n",
    "query = {'q': 'trump',  \n",
    "        'result_type': 'mixed',\n",
    "        'count': 40,\n",
    "        'lang': 'en',\n",
    "        \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a dictionary object of these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SteveSchmidtSES</td>\n",
       "      <td>Wed Aug 22 05:35:11 +0000 2018</td>\n",
       "      <td>This means Trump’s Presidency is not only ille...</td>\n",
       "      <td>67235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JoeNBC</td>\n",
       "      <td>Tue Aug 21 21:44:32 +0000 2018</td>\n",
       "      <td>Republicans trying to spin this as not involvi...</td>\n",
       "      <td>49189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acosta</td>\n",
       "      <td>Wed Aug 22 18:31:06 +0000 2018</td>\n",
       "      <td>Here is the cease and desist letter to Trump f...</td>\n",
       "      <td>26278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sufiy</td>\n",
       "      <td>Thu Aug 23 15:26:17 +0000 2018</td>\n",
       "      <td>RT @business: How seriously should people take...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>feefer2</td>\n",
       "      <td>Thu Aug 23 15:26:17 +0000 2018</td>\n",
       "      <td>RT @TeamPelosi: Yes Paul, we will. \\n\\nhttps:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user                            date  \\\n",
       "0   SteveSchmidtSES  Wed Aug 22 05:35:11 +0000 2018   \n",
       "2            JoeNBC  Tue Aug 21 21:44:32 +0000 2018   \n",
       "1            Acosta  Wed Aug 22 18:31:06 +0000 2018   \n",
       "38            Sufiy  Thu Aug 23 15:26:17 +0000 2018   \n",
       "37          feefer2  Thu Aug 23 15:26:17 +0000 2018   \n",
       "\n",
       "                                                 text  favorite_count  \n",
       "0   This means Trump’s Presidency is not only ille...           67235  \n",
       "2   Republicans trying to spin this as not involvi...           49189  \n",
       "1   Here is the cease and desist letter to Trump f...           26278  \n",
       "38  RT @business: How seriously should people take...               0  \n",
       "37  RT @TeamPelosi: Yes Paul, we will. \\n\\nhttps:/...               0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Search tweets\n",
    "dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}  \n",
    "for status in python_tweets.search(**query)['statuses']:  \n",
    "    dict_['user'].append(status['user']['screen_name'])\n",
    "    dict_['date'].append(status['created_at'])\n",
    "    dict_['text'].append(status['text'])\n",
    "    dict_['favorite_count'].append(status['favorite_count'])\n",
    "\n",
    "# Structure data in a pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(dict_)  \n",
    "df.sort_values(by='favorite_count', inplace=True, ascending=False)  \n",
    "df.head(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are only serachable for previous 7 days, no tweets older than that are shown in results. If we want to see the tweets moving forward in time we switch to the \"streaming\" version of this API.\n",
    "https://stackabuse.com/accessing-the-twitter-api-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streaming search\n",
    "\n",
    "from twython import TwythonStreamer  \n",
    "import csv\n",
    "\n",
    "# Filter out unwanted data\n",
    "def process_tweet(tweet):  \n",
    "    d = {}\n",
    "    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]\n",
    "    d['text'] = tweet['text']\n",
    "    d['user'] = tweet['user']['screen_name']\n",
    "    d['user_loc'] = tweet['user']['location']\n",
    "    return d\n",
    "\n",
    "\n",
    "# Create a class that inherits TwythonStreamer\n",
    "class MyStreamer(TwythonStreamer):     \n",
    "\n",
    "    # Received data\n",
    "    def on_success(self, data):\n",
    "\n",
    "        # Only collect tweets in English\n",
    "        if data['lang'] == 'en':\n",
    "            tweet_data = process_tweet(data)\n",
    "            self.save_to_csv(tweet_data)\n",
    "\n",
    "    # Problem with the API\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "    # Save each tweet to csv file\n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(r'saved_tweets.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(list(tweet.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the filter method to only collect tweets we're interested in. We'll create our filter with the track argument which provides the filter keywords, in our case \"daycare\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate from our streaming class\n",
    "stream = MyStreamer(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'],  \n",
    "                    creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])\n",
    "# Start the stream\n",
    "stream.statuses.filter(track='daycare')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data analysis with pandas\n",
    "\n",
    "import pandas as pd  \n",
    "tweets = pd.read_csv(\"saved_tweets.csv\")  \n",
    "tweets.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
